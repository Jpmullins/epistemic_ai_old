% Auto-generated from DOCX
\section{Implementation Considerations}
The theoretical framework is only useful if it can be implemented efficiently on top of LLMs. In this section, we discuss key practical considerations: ensuring the approach scales to the enormous proposition space of LLMs, dealing with partial observability and uncertainty, and integrating with the LLM's operation without requiring an entire logical reasoner that negates the advantages of the LLM.

\subsection{Scalability and Avoiding State Explosion}
One might fear that formalizing knowledge for an LLM (which can talk about virtually anything) would lead to an intractable state representation. After all, the set of possible ``worlds'' for an LLM that can generate arbitrary text is astronomically large. However, our approach is deliberately sparse. We never attempt to represent all those worlds; we only track the narrowing filter of what's been learned.

In practice, how many facts might an agent accumulate during a task? Possibly on the order of tens to low hundreds, even for complex tasks, because the agent will focus on task-relevant information. This is manageable. Each fact is likely a short sentence or predicate. Storing and checking a few hundred facts is trivial for a modern computer and can be kept within the LLM's context window if needed (especially as LLM context lengths grow). Moreover, if the number of facts grows too large, techniques like summarization can compress them. For example, if the agent has acquired a list of 20 clues in a murder mystery game, at some point it might synthesize them into higher-level theories and drop the minutiae.

Another angle to scalability is limiting reasoning depth. Full epistemic logic allows infinite chains like knowing that you know that you know... In our framework, we rarely need to go beyond one level (the agent knows X). We are not modeling another agent's mind in detail, so we don't have recursive knowledge except trivial common knowledge of the agent with itself. This dramatically simplifies things versus a general multi-agent epistemic model. We do not need to build huge Kripke structures; we just maintain a set of ground facts.

When it comes to inference, we also take a frugal approach. We are not employing an automated theorem prover to deduce all consequences of $\Sigma$. Instead, we rely on either simple direct checks or the LLM's own reasoning in most cases. For instance, to see if $\Sigma$ entails a certain condition, we could just prompt the LLM by providing $\Sigma$ and asking ``Given these facts, is it true that ...?'' The LLM is quite capable at basic logical reasoning when facts are explicitly given (and if it makes an error, we can double-check with a symbolic solver for safety on critical points).

Where needed, we can incorporate specific solvers for certain implications. For example, if some facts are numeric constraints, a linear solver could be used. But generally, since $\Sigma$ is small, even brute-force checking of combinations or using an SMT solver on it is feasible.

To connect with dynamic epistemic logic literature, Top et al. (2024) introduced the idea of bounded models in epistemic reasoning to deal with human subjects' limited reasoning depths\cite{pal-predictive}. Similarly, our agent is bounded: it will not consider arbitrarily complex hidden ramifications, only what it explicitly has logged or what it can derive with a few steps. In many cases, that's enough for practical performance, and it avoids the state space explosion of hypothetically considering everything.

\subsection{Partial Observability and Uncertainty}
Our framework inherently addresses partial observability: any fact not in $\Sigma$ is by default unknown to the agent. The agent may suspect or have probabilistic beliefs about unknowns, but until an announcement provides clarity, those possibilities remain open. In planning terms, the agent's belief state (the set of possible worlds consistent with $\Sigma$) often still contains multiple possibilities due to unknown factors. The agent can either act in a way that is safe despite the uncertainty (conformant planning) or perform sensing actions to reduce the uncertainty (conditional planning)\cite{pal-survey}.

One design is to explicitly mark certain propositions as unknown variables of interest, especially if we know the agent should resolve them. For example, if the task is to identify who among Alice or Bob has a secret, we could introduce a variable and note $\Sigma$ contains ``SecretHolder $\in \{Alice, Bob\}$'' initially. Then as evidence comes, that set narrows. This is akin to how programmers might maintain a list of suspects in code. In an LLM, one could simply ask the model to list possible candidates and update that list. Our approach gives a bit more rigor: the model can maintain in $\Sigma$ a disjunction or a set enumerating possibilities.

In terms of uncertainty, as discussed earlier, one could enrich $\Sigma$ with notations for belief degrees. For an initial implementation, one might not do full fuzzy logic but a simpler scheme: tag facts with confidence levels. E.g. ``(0.9) It is likely raining.'' If a conflicting fact comes, you might downgrade or replace. Some recent LLM self-evaluation techniques have the model assign likelihood scores to its statements. Those could feed into such tags. While we won't implement a full uncertain reasoning system here, it's good to note that nothing in the framework prevents storing ``probably X'' as a record. The query function then must interpret what to do if something is only probable.

If the environment can lie or be noisy, the agent could adopt a cautious approach: treat announcements as beliefs, not guaranteed truth, unless verified. For instance, if a tool is unreliable, the agent might add ``Tool said Y'' to $\Sigma$ rather than ``Y'', and keep a separate rule ``usually Tool is correct'' or a probability. Then it could cross-check via another tool or logic. This complicates things, but it shows the flexibility: $\Sigma$ can store not just direct facts but also source attributions or modal statements like $B (\text{Y})$ meaning ``the agent (just) believes Y tentatively''. A full dynamic doxastic logic (belief change) would allow modeling how beliefs get strengthened or weakened with evidence\cite{del-sep}. For now, in demonstration, we assume our sources are correct, so knowledge is monotonic.

One interesting challenge is knowledge forgetting or time. In a long-lived agent, facts in $\Sigma$ might become outdated (like step 5 in Table 1, where treasure moved, the old fact became invalid). One way to incorporate time is to timestamp facts or partition $\Sigma$ by context (like a world state that changes). A moving world is like a new scenario; announcements can be tagged as ``as of time t, $\phi$ is true''. Then if at time t+1 something changes, one can update accordingly. In some dynamic epistemic logics, there are action models for things like factual changes (which aren't announcements, but actual world changes that agents then observe). Our framework can tie into that by if the environment is known to change, the agent's planning should account for needing to re-check facts after certain actions. But this goes into planning strategy more than the epistemic record-keeping. Perhaps a simpler approach: when facts might have changed, the agent can remove them or mark them stale and re-investigate. This is similar to cache invalidation in software.

\subsection{Integration with LLM Workflows}
How do we implement this in an LLM agent pipeline? A concrete architecture can be sketched as follows:
\begin{enumerate}[leftmargin=*]
  \item Maintain a persistent \texttt{KnowledgeBase} object that stores $\Sigma$ across turns.
  \item For each reasoning cycle:
  \begin{enumerate}[label=\alph*.]
    \item Prepend a concise summary of $\Sigma$ to the LLM prompt or inject it via a system message.
    \item Instruct the model to respect the known facts (e.g. ``You know the following facts... Do not contradict them'').
    \item Let the LLM produce an action or answer; execute tool calls as needed and capture structured results.
    \item Normalise results into announcements and call \texttt{KnowledgeBase.update}.
    \item Optionally scan the model's own statements for confident self-announcements to append to $\Sigma$.
  \end{enumerate}
\end{enumerate}

Listing~\ref{lst:knowledge-base} shows a minimal Python helper that realises this loop.

\lstinputlisting[language=Python, caption={Knowledge base helper for epistemic announcements.}, label={lst:knowledge-base}]{code/knowledge_base.py}

The KnowledgeBase.update(info) needs some logic to interpret raw information into logical form. In some cases, it's straightforward (the tool returns structured data or a message we can map to a proposition). In others, if the LLM output says something like ``It might be A or B, not sure,'' we might encode that as uncertainty rather than a firm fact.

Using current tool-using agent frameworks (such as those built on top of LangChain or OpenAI functions), this integration is feasible: after each tool call, they often already inject the result into context. We'd additionally store it in a structured way.

Memory vs. KnowledgeBase: One might ask: the LLM has a context window and a kind of ``working memory'' in the prompt already; why not just rely on that for remembering things? The answer is twofold: (a) The context window is limited and might drop older info, whereas a separate knowledge store can persist arbitrarily. (b) More importantly, the knowledge base can be queried and manipulated symbolically. We can directly ask ``Does $\Sigma$ entail P?'' algorithmically, rather than relying on the LLM's sometimes unreliable recall. It's a form of verification and control. It also allows injecting absolute constraints (like if $\Sigma$ says ``Not X,'' we can forbid the model from outputting X as a solution, possibly by adding a high-weight penalty or an auxiliary classifier).

Interfacing with External Knowledge: If needed, the knowledge base could interface with external databases for domain knowledge. For example, if the agent needs to know capital cities, it might query Wiki and add ``Paris is capital of France'' to $\Sigma$. But one doesn't want to dump all Wikipedia; just relevant bits. This again highlights focusing on minimal relevant facts.

Epistemic Queries in Prompts: We can even have the LLM reason about its knowledge by asking it directly. For instance, ``Given the above facts, is it possible that ...?'' The LLM might say ``No, because fact X rules that out.'' This is a way to double-check its understanding. If it answered incorrectly, the system could intervene (maybe using a formal check or a second chain-of-thought) to correct it.

\subsection{Limits of Introspection and Resource Bounds}
While our framework gives structure, it doesn't magically make the LLM perfect. The model might still fail to use the knowledge correctly, especially under resource limitations or if the prompt becomes too long. We must ensure the prompt formatting is efficient and that the model is guided not to ignore $\Sigma$. Empirically, one might need to experiment with prompt styles (bullet lists of facts, or a brief narrative summary of known state) to see what the model best utilizes.

Another limitation is that the LLM might not always realize a needed knowledge gap without explicit training. By implementing these checks (like not proceeding if something isn't known), we sometimes override the model's own tendency. This is desirable for safety/consistency, but we should also allow the model's flexibility. The key is to strike a balance: enforce what's absolutely logically required, but let the model's creativity fill the rest. For instance, we wouldn't restrict the model's ability to generate hypotheses beyond $\Sigma$ (it can imagine things), but we would restrict it from stating those hypotheses as known facts unless they're deduced or verified.

Scalability not only refers to number of facts, but to the complexity of tasks (multi-step reasoning etc.). If tasks become extremely complex, the knowledge log might become large, but as argued, summarization or hierarchical reasoning can mitigate that. The agent could break a task into sub-tasks, each with its own sub-$\Sigma$ that gets merged or summarized into higher-level $\Sigma$.

Open-World vs Closed-World: Our approach implicitly assumes a kind of open-world scenario: not knowing $\varphi$ doesn't mean $\neg \varphi$ is assumed. The agent doesn't fill unknowns with defaults unless told. This is usually correct for real-world tasks. But sometimes a closed-world assumption is used (if something is not known true, assume false). We have to be careful: For example, if $\Sigma$ doesn't have ``key is present'', the agent shouldn't automatically assume ``key is not present''; it should recognize it as unknown and either ask or search for it. So by default we treat unknown as unknown. If needed, a particular domain can impose closed-world (like in a puzzle where anything not stated is false, but that's rarely the case in open environments).

Finally, we consider how this could be evaluated, as that guides some implementation choices. If we create an agent with this framework, we should test it on scenarios requiring consistent knowledge updates. Reflection-Bench tasks\cite{reflection-bench}, for example, include dimensions like belief updating and memory. We would expect our agent to outperform a baseline LLM agent on tasks where tracking what is known is crucial (like solving a mystery without contradicting clues, or doing a treasure hunt where you must remember which locations were searched). An evaluation might measure success rate, number of mistakes like redundant questions or contradictions, etc. We discuss more on this in the next section.

