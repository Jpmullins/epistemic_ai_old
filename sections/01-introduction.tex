% Auto-generated from DOCX
\section{Introduction}
Large Language Models (LLMs) have rapidly become central components in AI agents, powering complex decision-making and interactive workflows. However, a persistent challenge is how an LLM-based agent represents and updates its knowledge and beliefs about the world as it interacts. Current LLM agents often rely on implicit state in the prompt (e.g. chat history or scratchpad) and learned language patterns rather than an explicit, verifiable model of what the agent knows or believes at each step. This can lead to inconsistencies, hallucinations, or failures to recognize what information is missing. Recent evaluations of epistemic agency in LLMs, understood as the capacity to construct, adapt, and monitor beliefs, indicate that while modern LLMs show rudimentary abilities, they still have significant limitations in flexible belief updating and self-consistency\cite{reflection-bench}. In particular, advanced models struggle with higher-order reasoning and meta-reflection, i.e. reflecting on their own knowledge and correcting it when faced with new evidence\cite{reflection-bench}. These limitations motivate a more principled approach to managing an agent's state of knowledge in dynamic environments.

In this paper, we propose a practical, theoretically grounded framework for enabling belief and knowledge updates in LLM-based agents by drawing on epistemic logic, the logic of knowledge and belief, and its dynamic extension, Public Announcement Logic (PAL). Epistemic logic provides a rigorous way to describe an agent's knowledge in terms of possible worlds or states of affairs, and PAL describes how knowledge changes when new information is announced. The key idea is to treat events in an LLM agent's workflow (observations, tool outputs, intermediate conclusions, etc.) as analogous to announcements that update the agent's knowledge state. By formalizing these updates, we can maintain an explicit minimal representation of the agent's state, for example a log of facts learned or a summary of the current state, without needing to enumerate all possible world states or propositions. Each new piece of information eliminates incompatible possibilities from consideration, focusing the agent's belief state. Intuitively, to say an agent knows a fact means that fact holds in all situations the agent considers possible\cite{del-sep}. Conversely, acquiring new information that is certain and true can be seen as ruling out all previously possible situations where that information would be false\cite{del-sep}. Public announcements capture this process: a truthful announcement of a proposition $F$ causes all non-$F$ worlds to be discarded from every agent's set of epistemic possibilities\cite{del-sep}. Our framework brings this model-change semantics into LLM agents in a lightweight way by recording and enforcing the constraints introduced by each announcement (new information) rather than storing an explicit list of all possible worlds.

We emphasize minimality and practicality in the proposed approach. Rather than constructing full Kripke models with millions of possible worlds for an LLM's rich knowledge space, we maintain only the essential state updates, such as a list of facts confirmed or assumptions made, that serve as epistemic filters on the space of possibilities. For instance, if the agent receives a tool result confirming a fact, that fact is added to a knowledge base (or context window for the LLM), and any future reasoning implicitly treats worlds violating that fact as impossible. This is analogous to how in state estimation, obtaining one observation can exclude ``the vast majority of the universe'' of prior possibilities\cite{espf}, imposing ``hard bounds on the set of plausible states''\cite{espf}. The agent's knowledge state thus becomes a progressively narrower admissible region of possibilities consistent with everything it has observed or deduced so far, much like an epistemic support filter contracts a region of plausible states in response to new evidence\cite{espf}. Crucially, we do not require the agent to enumerate or generate all the eliminated possibilities explicitly; we only store the log of announcements (facts, observations, etc.) that have been accepted. This log or state summary implicitly defines the set of possible worlds remaining (those in which all logged facts are true). In effect, the agent's knowledge base is the conjunction of all announcements it has received, serving as a consistent, minimal record of its current knowledge. As long as this record is kept manageable in size (through techniques like summarization or forgetting of irrelevant details), the approach scales without the combinatorial explosion of explicitly tracking every hypothetical state.

The contributions of this work are as follows. First, we provide a self-contained overview of key concepts from epistemic logic and PAL that are relevant to AI agent design, including possible worlds semantics, knowledge vs. belief modalities, and public announcement updates, illustrated with concrete examples. We extend this with discussion of graded or fuzzy modal logics, which allow reasoning about degrees of belief or uncertainty, an important consideration for real-world LLM agents that operate under uncertainty and are not logically omniscient. Second, we propose a framework for an ``epistemic state management layer'' in an LLM agent architecture. This layer treats the agent's internal chain-of-thought and interactions as a sequence of epistemic actions (public announcements being the simplest) that update a compact state representation (like a knowledge log or set of constraints). We describe how this can guide the agent's action selection and reasoning: for example, the agent can query its knowledge state to decide if a certain fact is known or if further information-gathering is needed (akin to planning with knowledge preconditions\cite{Bolander2017Gentle}). We give implementation examples showing how tool calls and observations translate into public announcements that update the agent's state, and how minimal data structures (e.g. sets of propositions, or simple epistemic graphs) can simulate richer epistemic models. Third, we discuss how the framework can be implemented in practice without generating or storing all possible propositions an LLM might handle. We highlight techniques like representing knowledge states by formulas or constraints rather than explicit world lists, and using the LLM's own reasoning capability to infer consequences on the fly rather than precomputing all logical entailments. The agent is therefore resource-bounded and non-omniscient by design, acknowledging the practical limits on memory and computation. This stance aligns with real-world deployment and ``bounded rationality'' considerations\cite{pal-predictive}.

Finally, we position this work as a step toward structured cognition in LLM systems. By structured cognition, we mean an architecture where the flow of thought and action is modulated by an internal model of what is known, unknown, or conjectured, much like human reasoning keeps track of beliefs and uncertainties. Our epistemic update protocol can serve as a generic layer that could be added to various LLM agent frameworks (from conversational assistants to autonomous tool-using systems) to enhance consistency and reliability. We draw parallels to classical agent models like the belief-desire-intention (BDI) architecture, which explicitly maintains a set of beliefs (informational state) separate from goals and plans. In fact, the term belief is used (rather than ``knowledge'') in BDI to allow for the possibility that the agent's information might be wrong. Our framework similarly accounts for uncertainty and error: not every piece of information an LLM uses needs to be absolutely true, because some announcements might represent the agent's beliefs with some degree of confidence. We discuss how graded modal logic or probabilistic updating (as studied in dynamic epistemic logic extensions) can be incorporated to handle such cases. The overarching vision is that LLM agents can benefit from a logic-informed cognitive layer that ensures their evolving knowledge remains as consistent, minimal, and task-relevant as possible, even as they operate under partial observability and uncertainty.

The remainder of this paper is organized as follows. Section 2 (Background) reviews core concepts in epistemic logic, public announcement logic, and graded modal logics, grounding the discussion with simple examples and references to prior work in AI that combines logic and planning. Section 3 (Framework Design) presents our proposed approach to modeling LLM agent knowledge states and updates, including data structures for minimal epistemic state representation and algorithms for update and query. We illustrate the framework with an example workflow of an LLM agent interacting with a tool and updating its beliefs. Section 4 (Implementation Considerations) addresses practical issues such as partial observability, handling uncertainty (via fuzzy or probabilistic modalities), ensuring scalability through bounded reasoning and summarization, and integrating the logic-based state mechanism with the LLM's prompt or memory. Section 5 (Discussion) explores the implications of this approach, comparing it to related approaches (like chain-of-thought prompting and neuro-symbolic methods), and highlighting open challenges such as the limits of LLM introspection, potential inconsistencies, and how to evaluate epistemic reasoning in LLM agents. We suggest some directions for empirical evaluation, including adapting existing theory-of-mind or knowledge reasoning benchmarks to test LLM agents with and without epistemic update mechanisms. Finally, Section 6 (Conclusion) summarizes our findings and contributions. As a playful aside, we also propose a tentative name for this epistemically-informed agent framework, inspired by the rich tradition of logic and epistemology, to capture the spirit of the approach.

