% Auto-generated from DOCX
\section{Background: Epistemic Logic and Public Announcements}
\subsection{Knowledge, Belief, and Possible Worlds}
Epistemic logic is the branch of modal logic that formalizes reasoning about knowledge and belief. In epistemic logic, one typically introduces a modal operator $K_i$ to mean ``agent $i$ knows that...'', and similarly $B_i$ for ``agent $i$ believes that...'', in the context of multi-agent systems\cite{vanBenthem2011LogicalDynamics}. The semantics of these operators is often given by Kripke models (possible worlds models). A Kripke model for epistemic logic consists of a set of possible worlds (states of affairs), and for each agent $i$, an accessibility relation $R_i$ between worlds that encodes which worlds agent $i$ considers possible from a given world\cite{del-sep}. Intuitively, if the actual world is $w$, the set of worlds ${v \mid w R_i v}$ represents the epistemic alternatives that agent $i$ cannot distinguish from the actual world. Then $K_i \varphi$ (agent $i$ knows $\varphi$) is true at world $w$ if and only if $\varphi$ is true in all worlds $v$ accessible via $R_i$ from $w$\cite{del-sep}. In plainer terms: an agent knows $\varphi$ exactly when, given everything the agent has observed or been told, $\varphi$ holds in every situation the agent regards as possible. For example, if $\varphi$ means ``the key is in the drawer'' and the agent knows this, then any world the agent thinks might be the real one must have the key in the drawer; if there were even one conceivable world (consistent with the agent's information) where the key is elsewhere, the agent would not truly know $\varphi$.

The possible-worlds semantics elegantly captures the information content of knowledge. Knowledge rules out possibilities. As the agent gains more information, it rules out more worlds from consideration, thereby narrowing down the set of candidates for reality. Conversely, if an agent is uncertain about something, it means multiple possible worlds (differing on that fact) are still in its epistemic range. Knowledge in the standard modal logic treatment is usually assumed to be veridical (if $K_i \varphi$ then $\varphi$ is true) and agents are logically omniscient (they know all logical consequences of their knowledge). These assumptions correspond to properties of the accessibility relation like reflexivity, transitivity, and Euclideanness (characterizing the modal system S5 for knowledge)\cite{del-sep}. In practice, logical omniscience is an unrealistic idealization. Real agents (and LLMs) have bounded reasoning capabilities and may not instantly deduce all implications of what they know\cite{pal-predictive}. We will later relax these assumptions when mapping epistemic logic to practical LLM agents. The distinction between knowledge and belief is also important. Doxastic logic uses a modality $B_i$ for belief, which does not require truth (an agent can believe $\varphi$ even if $\varphi$ is false) and often corresponds to a weaker modal system (e.g. KD45 instead of S5). In AI agent architectures like BDI, a belief base represents the agent's informational state, acknowledging that some beliefs could be wrong or revised later. In our framework, we allow the LLM agent's state to include beliefs that might be probabilistic or defeasible, not just certainties, which provides a bridge to the graded logic discussion that follows.

To make these ideas concrete, consider a simple scenario: an agent trying to locate an object in one of two boxes. Initially, the agent doesn't know which box contains the object. This ignorance can be modeled by two possible worlds: $w_1$ (object in Box A) and $w_2$ (object in Box B), and the agent considers both $w_1$ and $w_2$ possible. Thus the agent does not know the object's location, because in one possible world the proposition ``object is in A'' is true, and in another it's false. Now suppose the agent looks into Box A and finds it empty. This observation eliminates all worlds where the object was in A (namely $w_1$) from the agent's epistemic alternatives. Now only $w_2$ remains possible, in which the object must be in B. At that moment, the agent knows the object is in Box B. We didn't need to explicitly list out every hypothetical scenario. It was enough to capture that the agent's prior knowledge allowed two possibilities, and after the observation only one. This example reflects how observations function as epistemic updates, ruling out possibilities and thereby converting uncertainty into knowledge. It is analogous to a logical inference as well: from ``Box A is observed empty'' the agent can infer ``object is in B'' given the prior that it's in A or B. The power of epistemic logic is in providing general tools to analyze such reasoning patterns, especially in multi-step or multi-agent settings where agents may reason about each other's knowledge (though in this paper we mostly focus on a single agent and the environment).

\subsection{Public Announcement Logic (PAL) and Knowledge Updates}
Public Announcement Logic (PAL) is a framework within dynamic epistemic logic that explicitly considers how knowledge changes when a truthful announcement is made publicly to all agents\cite{vanDitmarsch2007DEL,Baltag1998PAL}. The classic PAL operator is of the form $[!\psi]$ which, when prefixed to a formula $F$, means ``after a public announcement of $\psi$, $F$ holds.'' An announcement is assumed to be a truthful, transparently heard statement that all agents receive (and believe). The effect of an announcement $\psi$ in a Kripke model is to eliminate any possible world where $\psi$ is false\cite{del-sep}, because once $\psi$ is announced, all agents know $\psi$ is true (and know that others know, etc., though we won't delve deeply into common knowledge here). In the updated model, the set of worlds is shrunk to $W':={ w \in W \mid w \models \psi }$, and the accessibility relations are restricted accordingly. Thus, PAL provides a formal account of information-driven model transformations.

For example, consider the well-known Muddy Children Puzzle often cited in PAL contexts\cite{del-sep}: a group of children might each have mud on their forehead or not, and they can see others but not themselves. An announcement like ``At least one of you has muddy forehead'' (which they all hear) can, through its successive implications, lead them to deduce who is muddy after a few rounds of reasoning. PAL neatly models each announcement and its effect on the group's knowledge. For a simpler illustration, let's revisit the one-agent box scenario above in PAL terms. Initially, the agent doesn't know the object's location ($
\neg K(\text{in A})$ and $
\neg K(\text{in B})$ both hold). Now, the agent looking into Box A and seeing it empty can be treated as the environment making an announcement to the agent: ``Box A is empty.'' This announcement is effectively public to the agent's mind (though there is only one agent, ``public'' means the agent fully trusts the observation). Applying the PAL semantics, the agent's epistemic model is updated by dropping any world where ``Box A is empty'' is false, i.e., any world where the object was in A. The result is that in the updated model the agent knows ``object is in B.'' Formally, if $\psi$ is ``Box A is empty'' and $F$ is ``I know the object is in B,'' PAL would allow us to conclude $[!\psi] K(\text{in B})$. This is a rather trivial case, but it shows the mechanics: announcements reduce uncertainty by filtration.

One important aspect of PAL is that announcements can be about any proposition, including those describing agents' knowledge. This allows modeling higher-order knowledge updates (e.g. ``Announcing $\psi$'' might let agent 1 know that agent 2 now knows $\psi$, etc.). In our context of LLM agents, we mostly consider announcements of factual information from the environment or results of the agent's own actions. These are akin to sensing actions in AI planning, steps that reveal information about the world. Indeed, epistemic planning research often uses dynamic epistemic logic to model how an agent can perform actions that change not the physical world but the agent's knowledge state\cite{Bolander2017Gentle}. For instance, an epistemic planning problem might include an action ``inspect Box A'' that has the conditional effect of telling the agent whether Box A is empty or not. Such an action corresponds to a public announcement (to the agent itself) of the observation's outcome. Bolander (2017) notes that dynamic epistemic logic provides a natural and expressive way to formalize these scenarios, avoiding the need to hard-code knowledge changes in the logic of actions\cite{pal-survey}. Instead, the knowledge change falls out from the semantics of announcement: after the action, the agent's possible worlds are automatically restricted to those consistent with the observation.

In summary, PAL gives us a way to reason about belief state transformations due to new information. For a large language model agent, we can treat many events as announcements: - A tool API call result that returns some data can be an announcement of that data's value. - A user instruction or answer revealed is an announcement from the environment's perspective. - The agent's own conclusions or reflections could even be treated as (private) announcements to itself, solidifying a new piece of derived knowledge.

PAL assumes announcements are truthful and believed. In real settings, what if an agent receives misinformation or an observation that could be noisy? The standard PAL model would lead the agent to believe a falsehood, which might later need revision. Handling such cases leads into belief revision or more complex dynamic logics (e.g. plausibility models and action plausibility updates\cite{del-sep}). For now, we note that our framework can incorporate belief updates that are not certain truths by using a graded approach (discussed later). But the simplest case, which we focus on first, is where the agent treats incoming info as reliable (at least until evidence suggests otherwise).

\subsection{Graded and Fuzzy Modal Logics for Uncertainty}
Classical epistemic logic and PAL operate mostly in black-and-white terms: either a proposition is considered possible or it is eliminated; either something is known or it isn't. However, practical AI agents often deal with degrees of belief or uncertainty. A large language model might have a probability or confidence distribution over possible answers, or might consider some facts more plausible than others given its prior knowledge. To capture this within a logical framework, researchers have developed graded modal logics and fuzzy logic extensions of epistemic logic. The basic idea is to allow statements like ``agent $i$ believes $\varphi$ with at least degree $0.8$'' or ``with probability at least $0.8$.'' A graded modal operator might be written $B_i^{\ge k} \varphi$ meaning ``agent $i$ believes $\varphi$ with degree at least $k$'' on some scale. In other approaches, one can attach a numerical plausibility or probability to each possible world, instead of a binary possible/impossible classification\cite{del-sep}.

One influential framework is the use of plausibility models in dynamic epistemic logic for belief\cite{del-sep}. In a plausibility model, each agent's uncertainty is represented by a ranking or partial order over worlds (rather than just an accessibility set)\cite{del-sep}. For example, an agent might consider many worlds possible, but some are deemed more plausible than others. A belief could then be defined as truth in all the most plausible worlds (typically formalized via a ``selection function'' or spheres of plausibility around the actual world, following Grove's system of spheres analogy\cite{del-sep}). When new information arrives that is uncertain. For example, the agent may encounter situations where the event is not a guaranteed truthful announcement but an observation with some chance of error, the agent can update plausibilities rather than completely eliminate worlds. If the agent is told ``$\psi$ is likely true,'' it might boost the plausibility of worlds where $\psi$ holds, without entirely discarding $
eg\psi$ worlds.

Another approach is probabilistic epistemic logic, which assigns probabilities to sets of worlds and allows formulas like ``agent $i$ assigns at least 80\% probability to $\varphi$.'' Dynamic epistemic logics have been extended with Bayesian update operators to model agents performing Bayesian belief updates on their probability distributions when events occur\cite{del-sep}. For instance, after observing some evidence, an agent might update the probabilities of various possibilities according to Bayes' rule. This is analogous to a ``soft'' announcement which reduces probability of some worlds to zero (if the evidence has zero probability in them) and reweights others proportionally.

In the context of LLM agents, why do graded or fuzzy modalities matter? Because LLMs do not intrinsically operate with a clear true/false world model. Instead, an LLM has an internal representation (in its weights and activations) that can be seen as encoding likelihoods or confidence about various statements. When an LLM ``considers'' a question, it often produces probabilities for different answers. We could interpret an LLM's 90\% confidence in an answer as the agent believing that answer with degree 0.9. If the agent then finds conflicting evidence, it might lower that confidence. Representing these changes in a logical way could involve saying ``Belief(p) dropped from 0.9 to 0.1 after observing X.'' While our framework in this paper primarily treats the knowledge state in a symbolic fashion (for clarity of exposition), it can be generalized to allow weighted beliefs. For example, we might maintain not just a set of known facts, but a set of believed propositions with confidence scores. Updates would then use rules akin to Dempster-Shafer evidence combination or Bayesian updates rather than simple set subtraction.

There are formalisms specifically for fuzzy modal logic that allow truth values in $[0,1]$. For instance, Li and Gong (2022) present a graded many-valued modal logic G(S5) with definitions of graded truth and rough truth degrees for modal formulas\cite{graded-many-valued}. Such formalisms could be applied to model an LLM agent's partial knowledge. Imagine the agent has a fuzzy belief like ``I suspect with 0.7 confidence that the user is asking about sports.'' This might be represented in a graded epistemic logic and updated as the conversation continues.

For practical implementation, one need not fully formalize a probability distribution. A simpler method used in some agent systems is to keep multiple possible hypotheses around. For instance, an agent might have a shortlist of plausible worlds or scenarios it's considering, rather than one fixed believed world. This connects to the idea of hypothetical or counterfactual reasoning; the agent might say ``If hypothesis H were true, I'd expect observation O; since I observed not-O, H becomes less plausible.'' Some LLM-based reasoning frameworks attempt to do this implicitly by prompting the model to consider alternatives (``let's assume X, see if it leads to contradiction''). Our logic-based approach could make it more systematic: the state could be a set of possibilities with labels like ``eliminated'' or ``active'' and perhaps a score. A graded announcement could then downgrade the plausibility of worlds failing the announcement without fully deleting them, reflecting uncertainty.

In summary, graded epistemic logics provide the tools to represent uncertainty and belief strength, which are crucial for realistic agent reasoning under partial information. In this paper, we will mostly illustrate using the simpler crisp announcements (which either completely rule things out or not) for clarity. But it should be kept in mind that this can be generalized. The framework we propose can function in a ``belief mode'' where announcements act like Bayesian evidence: rather than an all-or-nothing elimination, an update could mark some knowledge as uncertain or likely. For instance, if an LLM agent reads a single article claiming something controversial, it might not fully ``know'' that claim as true, but it might assign it a belief status pending further verification. Implementing this might mean the agent's knowledge log records the claim with a note ``unverified'' or a confidence level, and the decision-making layer treats it accordingly (e.g. it might seek a second source to confirm, rather than acting on it blindly).

\subsection{LLM Agents and Existing Approaches to State Management}
Before diving into our framework, it is useful to review how current LLM-based agent systems handle state and memory, and where the gaps are. Traditional AI planning systems had explicit world models and belief states, but many modern LLM agents use a mostly implicit approach:
\begin{itemize}
  \item \textbf{Prompt history as memory.} The conversation or action history is kept as text and provided to the LLM in each prompt (possibly truncated or summarized if it grows too long). This gives the model context but leaves the information unstructured, so the LLM must infer which statements are still valid.
  \item \textbf{Tool-assisted reasoning transcripts.} Frameworks such as ReAct interleave thoughts and tool calls, storing them in a transcript that acts as a log of what the agent has done and learned. Because the transcript lacks enforced logical constraints, consistency depends entirely on the LLM's learned behavior.
  \item \textbf{Long-term memory modules.} Cognitive architectures like Generative Agents maintain evolving memory streams with summarization and retrieval components\cite{generative-agents}. They store events rather than a full world state, which aligns with our minimal representation, yet they still rely on prompting the LLM to infer the implications of new observations. Our framework can provide the formal backbone that automatically records consequences such as marking $X$ as completed when an observation says ``X is now done.''
  \item \textbf{Symbolic augmentations.} Other work combines LLMs with knowledge graphs or symbolic modules so that facts can be stored and queried more reliably than in the model's parameters. These systems demonstrate the value of a factual database; we generalize this idea by maintaining the ``database of known facts'' under announcement-style update rules, including meta-facts about unknowns or assumptions.
\end{itemize}

These patterns show that current systems rely on memory mechanisms without a formally specified update semantics.

In the planning and multi-agent systems community, epistemic planning has tackled the problem of agents planning under uncertainty and with information-producing actions\cite{pal-survey,dynamic-term-modal}. Techniques include compiling epistemic logic into classical planning via state enumeration or using satisfiability solvers on epistemic formulas. However, these techniques often struggle with scale because the space of possible knowledge states can blow up combinatorially (especially with nested beliefs about other agents). LLM agents differ in that we have a powerful pattern-matching and generative core (the LLM) that can do a lot of the heavy lifting if guided properly, rather than having to explicitly enumerate states. This suggests a hybrid approach: use logic to maintain a coarse but sound state representation (prevent obvious contradictions, know what needs to be known, etc.), and use the LLM for the ``heavy'' reasoning and language generation conditioned on that state.

To underscore the need for explicit state tracking, consider an example from a dialog or interactive fiction setting. Without an explicit state, an LLM may forget a fact stated only a while ago, or it may inadvertently introduce inconsistencies (like mentioning an object that was taken earlier as if it's still present). Humans maintain an internal model of the conversation's common ground, a form of common knowledge, to avoid such mistakes. Our framework effectively gives the LLM a pseudo-common-ground model: the log of announcements can be thought of as maintaining common knowledge between the agent and itself (and the user, if applicable) of what has been said or observed. PAL includes common knowledge, formalized as the greatest fixpoint of iterated ‘everyone knows’ ($E^*$) over a group. For a single agent, under positive introspection (axiom 4), common knowledge of $\varphi$ is equivalent to $K\varphi$; without 4 it is stronger than a single $K\varphi$.

In conclusion, existing LLM agent methodologies implicitly recognize the importance of remembering and updating state (through logs, memories, context windows, etc.), but they often lack a formal semantics for those updates. This can lead to either oversights (the agent doesn't realize what it should logically infer) or hallucinations (the agent asserts something inconsistent because it failed to notice a contradiction). By importing the principles of epistemic logic and PAL, we aim to provide a clear semantics for the agent's memory and a protocol for updating it that ensures logical consistency in a minimal, tractable way.

